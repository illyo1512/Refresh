import requests
from bs4 import BeautifulSoup
import os

# ì½”ë“œìš”ì•½ urlë§Œ í•œë²ˆ ì­‰ ë°›ì•„ì˜¤ê¸° -> ì•ˆì˜ ë³¸ë¬¸ ë‚´ìš© ë°›ì•„ì˜¤ê¸°  ì œëª© url ë³¸ë¬¸ ë”°ë¡œ ì €ì¥ txtíŒŒì¼ë¡œ ë˜ê²Œí•´ë‘”ê±° ë¦¬ìŠ¤íŠ¸ë¡œ ìˆ˜ì •í•„ìš”


# âœ… ë§ˆì§€ë§‰ í¬ë¡¤ë§í•œ ìµœì‹  ê¸°ì‚¬ URL ë¡œë”©
LAST_URL_FILE = "/content/last_crawled_url.txt"
def load_last_url():
    if os.path.exists(LAST_URL_FILE):
        with open(LAST_URL_FILE, 'r', encoding='utf-8') as f:
            return f.read().strip()
    return None

def save_last_url(url):
    with open(LAST_URL_FILE, 'w', encoding='utf-8') as f:
        f.write(url.strip())

# âœ… ê¸°ì‚¬ URL ìˆ˜ì§‘ í•¨ìˆ˜ (ì´ì „ URL ë„ë‹¬ ì‹œ ì¤‘ë‹¨)
def fetch_article_urls(base_url, last_url=None, max_count=30):
    response = requests.get(base_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    article_links = []
    article_titles = []

    for a_tag in soup.select('a.sa_text_title'):
        href = a_tag.get('href')
        title = a_tag.get_text(strip=True)
        if href and href.startswith("https://"):
            if href == last_url:
                print("ğŸ›‘ ë§ˆì§€ë§‰ í¬ë¡¤ë§ ìœ„ì¹˜ ë„ë‹¬, ì¤‘ë‹¨")
                break
            article_links.append(href)
            article_titles.append(title)
        if len(article_links) >= max_count:
            break

    return list(zip(article_links, article_titles))

# âœ… ë³¸ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜
def fetch_article_body(url):
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')

        content = soup.find("div", class_="go_trans _article_content")
        if not content:
            content = soup.find("article")

        if content:
            return content.get_text(separator='\n', strip=True)
        else:
            return "âŒ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {e}"

# âœ… ì „ì²´ ì‹¤í–‰ ë° ì €ì¥

def run_and_save_articles():
    base_url = "https://news.naver.com/breakingnews/section/102/249"
    last_url = load_last_url()

    articles = fetch_article_urls(base_url, last_url=last_url)
    if not articles:
        print("ğŸš« ìƒˆ ê¸°ì‚¬ ì—†ìŒ")
        return

    save_dir = "/content/naver_articles"
    os.makedirs(save_dir, exist_ok=True)

    # ìµœì‹  URL ì €ì¥
    save_last_url(articles[0][0])

    for idx, (url, title) in enumerate(articles):
        number_part = str(idx + 1)

        print(f"[{number_part}] Fetching: {url}")
        body = fetch_article_body(url)

        # âœ… íŒŒì¼ ì €ì¥
        with open(os.path.join(save_dir, f"{number_part}_url.txt"), 'w', encoding='utf-8') as f:
            f.write(url)
        with open(os.path.join(save_dir, f"{number_part}_title.txt"), 'w', encoding='utf-8') as f:
            f.write(title)
        with open(os.path.join(save_dir, f"{number_part}_article.txt"), 'w', encoding='utf-8') as f:
            f.write(body)

        print(f"âœ… ì €ì¥ ì™„ë£Œ: {number_part}_*.txt")

# âœ… ì‹¤í–‰
if __name__ == "__main__":
    run_and_save_articles()
