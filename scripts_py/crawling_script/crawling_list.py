import requests
from bs4 import BeautifulSoup
import os
import sys
from pathlib import Path

# ê²½ë¡œ ì„¤ì •ì„ ìœ„í•´ config ë””ë ‰í† ë¦¬ë¥¼ Python pathì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent.parent))
from config.paths import paths, setup_directories

# âœ… headers ì„¤ì •
headers = {
    'User-Agent': 'Mozilla/5.0'
}

# âœ… ë§ˆì§€ë§‰ í¬ë¡¤ë§í•œ ìµœì‹  ê¸°ì‚¬ URL ë¡œë”©
def load_last_url():
    if paths.last_crawled_url_file.exists():
        with open(paths.last_crawled_url_file, 'r', encoding='utf-8') as f:
            return f.read().strip()
    return None

def save_last_url(url):
    # ë””ë ‰í† ë¦¬ê°€ ì—†ìœ¼ë©´ ìƒì„±
    paths.ensure_dir_exists(paths.crawling_data_dir)
    with open(paths.last_crawled_url_file, 'w', encoding='utf-8') as f:
        f.write(url.strip())

# âœ… ê¸°ì‚¬ URL ìˆ˜ì§‘ í•¨ìˆ˜ (ì´ì „ URL ë„ë‹¬ ì‹œ ì¤‘ë‹¨)
def fetch_article_urls(base_url, last_url=None, max_count=30):
    response = requests.get(base_url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')

    article_links = []
    article_titles = []

    for a_tag in soup.select('a.sa_text_title'):
        href = a_tag.get('href')
        title = a_tag.get_text(strip=True)
        if href and href.startswith("https://"):
            if href == last_url:
                print("ğŸ›‘ ë§ˆì§€ë§‰ í¬ë¡¤ë§ ìœ„ì¹˜ ë„ë‹¬, ì¤‘ë‹¨")
                break
            article_links.append(href)
            article_titles.append(title)
        if len(article_links) >= max_count:
            break

    return list(zip(article_links, article_titles))

# âœ… ë³¸ë¬¸ ìˆ˜ì§‘ í•¨ìˆ˜
def fetch_article_body(url):
    try:
        res = requests.get(url, headers=headers)
        soup = BeautifulSoup(res.text, 'html.parser')

        content = soup.find("div", class_="go_trans _article_content")
        if not content:
            content = soup.find("article")

        if content:
            return content.get_text(separator='\n', strip=True)
        else:
            return "âŒ ë³¸ë¬¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
    except Exception as e:
        return f"âŒ ì˜¤ë¥˜: {e}"

# âœ… ìŠ¤í”„ë§ë¶€íŠ¸ í˜¸ì¶œìš© í•¨ìˆ˜ (ë¦¬ìŠ¤íŠ¸ ë°˜í™˜)
def run_and_return_articles():
    # í•„ìš”í•œ ë””ë ‰í† ë¦¬ ì´ˆê¸°í™”
    setup_directories()
    
    base_url = "https://news.naver.com/breakingnews/section/102/249"
    last_url = load_last_url()

    articles = fetch_article_urls(base_url, last_url=last_url)
    if not articles:
        return []

    # ìµœì‹  URL ì €ì¥
    save_last_url(articles[0][0])

    result = []
    for idx, (url, title) in enumerate(articles):
        body = fetch_article_body(url)
        result.append({
            "number": idx + 1,
            "url": url,
            "title": title,
            "content": body
        })

    return result

# âœ… í…ŒìŠ¤íŠ¸ìš© standalone ì‹¤í–‰
def main():
    articles = run_and_return_articles()
    for a in articles:
        print("=" * 60)
        print(f"[{a['number']}] {a['title']}")
        print(a['url'])
        print(a['content'][:300], "...")

if __name__ == "__main__":
    main()
